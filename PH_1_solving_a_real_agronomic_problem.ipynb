{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8993cd",
   "metadata": {},
   "source": [
    "# Import the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc90fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import mlflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils.utilities import load_checkpoint, save_checkpoint\n",
    "from utils.metrics import IntegratedEvaluator\n",
    "import pyarrow.parquet as pq\n",
    "from omegaconf import OmegaConf\n",
    "from datasets.dataOps import create_ood_datasets, create_datasets\n",
    "from hydra.utils import instantiate\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = \"5c6f56f74c4146f4b2aedc6a9546816f\" # Put here your running id for the S2S_{msd} experiment on the D_{ood-GP} partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc59824",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "client = mlflow.client.MlflowClient()\n",
    "dico = client.get_run(RUN_ID).to_dictionary()\n",
    "print(dico[\"data\"][\"tags\"][\"exp_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(os.path.join(dico[\"info\"][\"artifact_uri\"].removeprefix(\"file://\"), \"config_exp.yaml\"))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "data = {}\n",
    "for array in [\"static_data\", \"before_ts\", \"after_ts\", \"target_ts\", \"mask_target\", \"cat_dicos\"]:\n",
    "    with open(f\"{cfg.raw_data_folder + array}.pkl\", \"rb\") as f:\n",
    "        data[array] = pickle.load(f)\n",
    "table = pq.read_table(cfg.raw_data_folder + cfg.info_ts_file)\n",
    "ids = table.to_pandas().index.to_list()\n",
    "list_unic_cat = [len(dico.keys()) for dico in data[\"cat_dicos\"].values()]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(ids=ids,\n",
    "                                                           static_data=data[\"static_data\"],\n",
    "                                                           before_ts=data[\"before_ts\"],\n",
    "                                                           after_ts=data[\"after_ts\"],\n",
    "                                                           target_ts=data[\"target_ts\"],\n",
    "                                                           mask_target=data[\"mask_target\"],\n",
    "                                                           train_size=cfg.training.train_size,\n",
    "                                                           val_size=cfg.training.val_size,\n",
    "                                                           raw_data_folder=cfg.raw_data_folder,\n",
    "                                                           means_and_stds_path=cfg.means_and_stds_path,\n",
    "                                                           )\n",
    "\n",
    "encoder = instantiate(cfg.model.encoder,\n",
    "                        list_unic_cat=list_unic_cat).to(device)\n",
    "decoder = instantiate(cfg.model.decoder).to(device)\n",
    "\n",
    "optimizer = instantiate(cfg.training.optimizer,\n",
    "                        params = list(encoder.parameters()) + list(decoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charge trained model\n",
    "checkpoint = torch.load(dico[\"info\"][\"artifact_uri\"].removeprefix(\"file://\") + \"/best_model/best_model.pth\")\n",
    "\n",
    "load_checkpoint(checkpoint,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze encoder and decoder\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in decoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare modified data \n",
    "\n",
    "selected_id = [\"011a0bab-5f91-4784-a7b5-eb895dcdbcda\"]\n",
    "indices_to_use = [\n",
    "    i for i, item in enumerate(train_dataset)\n",
    "    if item['id'] in selected_id\n",
    "]\n",
    "\n",
    "# Create a Subset of the dataset using only the objective observations\n",
    "subset_dataset = Subset(train_dataset, indices_to_use)\n",
    "loader = DataLoader(subset_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e24ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    print(batch[\"static_data_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1635f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/work_data/means_and_stds.pkl\", \"rb\") as f:\n",
    "    means_and_stds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 exp_name,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 learning_rate,\n",
    "                 num_epochs,\n",
    "                 train_dataloader,\n",
    "                 checkpoints_path,\n",
    "                 device):\n",
    "\n",
    "        # Core components\n",
    "        self.exp_name = exp_name\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.device = device\n",
    "\n",
    "        # Data\n",
    "        self.train_dataloader = train_dataloader\n",
    "\n",
    "        # Paths\n",
    "        self.checkpoints_path = checkpoints_path\n",
    "\n",
    "        # Training options\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Internal trackers\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_epoch(self,\n",
    "                    epoch,\n",
    "                    num_epochs):\n",
    "            \n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        total_epoch_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(self.train_dataloader):\n",
    "\n",
    "            new_data = {}    \n",
    "            for array in [\"before_ts\", \"after_ts\"]:\n",
    "                with open(f\"data/work_data/RCP_85/{array}.pkl\", \"rb\") as f:\n",
    "                    new_data[array] = pickle.load(f)\n",
    "\n",
    "            total_batch_loss = 0.0\n",
    "            outputs = []\n",
    "            static_data_cat = batch[\"static_data_cat\"].to(self.device)\n",
    "            before_ts = torch.tensor((new_data[\"before_ts\"] - means_and_stds[\"before_ts_mean\"]) / means_and_stds[\"before_ts_std\"], dtype=torch.float32).to(self.device) \n",
    "            after_ts = torch.tensor((new_data[\"after_ts\"] - means_and_stds[\"after_ts_mean\"]) / means_and_stds[\"after_ts_std\"], dtype=torch.float32).to(self.device)\n",
    "            target_ts = batch[\"target_ts\"].to(self.device)\n",
    "            mask_target = batch[\"mask_target\"].to(self.device)\n",
    "\n",
    "            mask = torch.zeros_like(self.static_data_num)\n",
    "            mask[:, [3,4]] = 1.0\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            latent, x_t = self.encoder(self.static_data_num, static_data_cat, before_ts)\n",
    "            x = torch.cat([x_t.unsqueeze(1), target_ts[:, :-1, :]], dim=1)\n",
    "            h_0 = latent  # h_0\n",
    "            outputs, _ = self.decoder(x, h_0, after_ts, ar=False)\n",
    "\n",
    "            last_idx = mask_target.sum(dim=1) - 1          # (B,)\n",
    "\n",
    "            total_batch_loss = -outputs[0, last_idx, 3]\n",
    "\n",
    "            total_batch_loss.backward()\n",
    "            self.static_data_num.grad *= mask\n",
    "            self.optimizer.step()\n",
    "            print(self.static_data_num)\n",
    "\n",
    "            total_epoch_loss += total_batch_loss.item()\n",
    "\n",
    "        print(f\"TRAIN : Epoch [{epoch+1}/{num_epochs}], Loss: {total_epoch_loss:.4f}\")\n",
    "\n",
    "        ckpt_path = f\"{self.checkpoints_path}/checkpoint.pth\"\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch+1,\n",
    "            \"state_encoder_dict\": self.encoder.state_dict(),\n",
    "            \"state_decoder_dict\": self.decoder.state_dict(),\n",
    "            }\n",
    "        save_checkpoint(checkpoint, filename=ckpt_path)\n",
    "\n",
    "        return total_batch_loss\n",
    "\n",
    "    def train_loop(self, loader):\n",
    "        batch = next(iter(loader))\n",
    "        self.static_data_num = batch[\"static_data_num\"].to(self.device)\n",
    "        self.static_data_num = self.static_data_num.clone().detach().requires_grad_(True)\n",
    "        print(self.static_data_num)\n",
    "        self.optimizer = torch.optim.Adam(params=[self.static_data_num], lr=self.learning_rate)\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            agr_yield = self.train_epoch(epoch, self.num_epochs)\n",
    "        return self.static_data_num, agr_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eddc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(name=\"agronomic/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b32fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(exp_name=\"Agronomic_inverse_problem\",\n",
    "                  encoder=encoder,\n",
    "                  decoder=decoder,\n",
    "                  learning_rate=0.001,\n",
    "                  num_epochs=100,\n",
    "                  train_dataloader=loader,\n",
    "                  checkpoints_path=\"agronomic/\",\n",
    "                  device=device,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa11c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector, agr_yield = trainer.train_loop(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector[0][-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(agr_yield * -1 * means_and_stds[\"target_ts_std\"][3]) + means_and_stds[\"target_ts_mean\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "theta = np.arctan2(vector[0][-2].item(), vector[0][-1].item())\n",
    "\n",
    "# wrap angle to [0, 2Ï€)\n",
    "theta = np.mod(theta, 2 * np.pi)\n",
    "# recover day of year\n",
    "day = theta * (365 / (2 * np.pi))\n",
    "\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89771ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.strptime('2035 350', '%Y %j')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "winn_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
